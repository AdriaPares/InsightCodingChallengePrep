{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/indexing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in listdir(directory)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['32',\n",
       " '35',\n",
       " '34',\n",
       " '33',\n",
       " '20',\n",
       " '18',\n",
       " '27',\n",
       " '9',\n",
       " '0',\n",
       " '11',\n",
       " '7',\n",
       " '29',\n",
       " '16',\n",
       " '42',\n",
       " '6',\n",
       " '28',\n",
       " '17',\n",
       " '1',\n",
       " '10',\n",
       " '19',\n",
       " '26',\n",
       " '8',\n",
       " '21',\n",
       " '44',\n",
       " '43',\n",
       " '38',\n",
       " '36',\n",
       " '31',\n",
       " '30',\n",
       " '37',\n",
       " '39',\n",
       " '41',\n",
       " '24',\n",
       " '23',\n",
       " '4',\n",
       " '15',\n",
       " '3',\n",
       " '12',\n",
       " '40',\n",
       " '2',\n",
       " '13',\n",
       " '5',\n",
       " '14',\n",
       " '22',\n",
       " '25']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_dict:\n",
    "    def __init__(self):\n",
    "        self.last = 0\n",
    "        self.dict = {}\n",
    "    def add(self, key):\n",
    "        self.dict[key] = self.last\n",
    "        self.last += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Word_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_dictionary(line, word_dict):\n",
    "    for word in re.findall(r'\\w+', line):\n",
    "        if word not in word_dict.dict.keys():\n",
    "            word_dict.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Word_dict()\n",
    "for file_name in files:\n",
    "    with open(directory+file_name, encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            # print(line)\n",
    "            add_to_dictionary(line, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115128"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words.dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import re\n",
    "from pyspark.sql.functions import input_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-43-2dfc28fca47d>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-2dfc28fca47d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-43-2dfc28fca47d>:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------------------+\n",
      "|label|raw            |vectors                        |\n",
      "+-----+---------------+-------------------------------+\n",
      "|0    |[a, b, c, d]   |(5,[0,1,2,4],[1.0,1.0,1.0,1.0])|\n",
      "|1    |[a, e, b, c, a]|(5,[0,1,2,3],[2.0,1.0,1.0,1.0])|\n",
      "+-----+---------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.createDataFrame([(0, [\"a\", \"b\", \"c\", \"d\"]), (1, [\"a\", \"e\", \"b\", \"c\", \"a\"])],[\"label\", \"raw\"])\n",
    "cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "model = cv.fit(df)\n",
    "model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             vectors|\n",
      "+--------------------+\n",
      "|(5,[0,1,2,4],[1.0...|\n",
      "|(5,[0,1,2,3],[2.0...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).select('vectors').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/mini_reverse'\n",
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_builder(word):\n",
    "    global a\n",
    "    a += 1\n",
    "    return (word,a-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./data/mini_reverse MapPartitionsRDD[196] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile(folder)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 1),\n",
       " ('General', 1),\n",
       " ('Kenobi', 1),\n",
       " ('needs', 1),\n",
       " ('of', 1),\n",
       " ('came', 1),\n",
       " ('say', 1),\n",
       " ('the', 1),\n",
       " ('general', 1),\n",
       " ('to', 1),\n",
       " ('hello', 1),\n",
       " ('Hello', 1),\n",
       " ('World', 1),\n",
       " ('am', 1),\n",
       " ('new', 1),\n",
       " ('The', 1),\n",
       " ('world', 1),\n",
       " ('more', 1),\n",
       " ('I', 1),\n",
       " ('disagree', 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.flatMap(lambda x: re.findall(r'\\w+', x)).map(lambda x: (x,1)).reduceByKey(lambda x,y: max(x,y)).collect()\n",
    "#reduceByKey(lambda x,y: max(x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'General',\n",
       " 'Kenobi',\n",
       " 'needs',\n",
       " 'of',\n",
       " 'came',\n",
       " 'say',\n",
       " 'the',\n",
       " 'general',\n",
       " 'to',\n",
       " 'hello',\n",
       " 'Hello',\n",
       " 'World',\n",
       " 'am',\n",
       " 'new',\n",
       " 'The',\n",
       " 'world',\n",
       " 'more',\n",
       " 'I',\n",
       " 'disagree']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.flatMap(lambda x: re.findall(r'\\w+', x)).distinct().collect()#.withColumn(\"uniqueID\",monotonicallyIncreasingId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|raw                                                                                                                                                          |vectors                                                                                                                                   |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[Hello, there, General, Kenobi, Hello, World, I, am, the, new, general, The, world, needs, more, of, Kenobi, I, disagree, The, general, came, to, say, hello]|(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = sql.createDataFrame([(0, text.flatMap(lambda x: re.findall(r'\\w+', x)).collect())], ['label','raw'])\n",
    "cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "model2 = cv.fit(df2)\n",
    "test_vector = model2.transform(df2)\n",
    "test_vector.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = text.flatMap(lambda file,contents:[(file, word) for word in contents.lower().split()])\\\n",
    "            .map(lambda word: (word, 1))\\\n",
    "            .map(lambda file, word: (word,[file]))\\\n",
    "            .reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/Users/luzjubierrezapater/insight_projects/interview_prep/coding_challenges/coding_challenges/data/mini_reverse/0',\n",
       "  'Hello there, General Kenobi'),\n",
       " ('file:/Users/luzjubierrezapater/insight_projects/interview_prep/coding_challenges/coding_challenges/data/mini_reverse/1',\n",
       "  'Hello World, I am the new general'),\n",
       " ('file:/Users/luzjubierrezapater/insight_projects/interview_prep/coding_challenges/coding_challenges/data/mini_reverse/3',\n",
       "  'The world needs more of Kenobi.\\nI disagree'),\n",
       " ('file:/Users/luzjubierrezapater/insight_projects/interview_prep/coding_challenges/coding_challenges/data/mini_reverse/2',\n",
       "  'The general came to say hello')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_text = sc.wholeTextFiles(folder)\n",
    "whole_text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'saveAsTextFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-0b0fec954c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mflatMapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'saveAsTextFile'"
     ]
    }
   ],
   "source": [
    "whole_text.map(lambda x: (x[0].split('/')[-1], re.findall(r'\\w+', x[1])))\\\n",
    ".flatMapValues(lambda x: x)\\\n",
    ".map(lambda x: (x[1],list(x[0])))\\\n",
    ".reduceByKey(lambda x,y: x+y)\\\n",
    ".saveAsTextFile('output.txt')\n",
    "#.collect()\n",
    "\n",
    "#.flatMap(lambda file,contents: [(file, word) for word in contents]).collect()\n",
    "\n",
    "#.map(lambda x: (re.findall(r'\\w+', x[1]),x[0].split('/')[-1])).flatMap(lambda x: ).collect()\n",
    "\n",
    "\n",
    "\n",
    "#.map(lambda x: (re.findall(r'\\w+', x[0]),x[1])).select(explode())\n",
    "\n",
    "#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
